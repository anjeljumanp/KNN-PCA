{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric and instance-based (lazy learning) algorithm, meaning it does not build an explicit model during training. Instead, it stores the entire training dataset and makes predictions only when a new data point needs to be classified or predicted.\n",
        "\n",
        "KNN works based on the idea of similarity. When a new data point is given, the algorithm calculates the distance between this point and all the points in the training dataset. Common distance measures include Euclidean distance, Manhattan distance, or Minkowski distance. After computing distances, the algorithm selects the **K** nearest data points (neighbors) based on the chosen distance metric.\n",
        "\n",
        "In **classification problems**, KNN assigns the new data point to the class that is most common among its K nearest neighbors. This is known as majority voting. For example, if K = 5 and among the five nearest neighbors, three belong to Class A and two belong to Class B, the new data point is classified as Class A.\n",
        "\n",
        "In **regression problems**, instead of majority voting, KNN calculates the average (or sometimes weighted average) of the values of the K nearest neighbors. The predicted value is the mean of those neighbors’ target values.\n",
        "\n",
        "The choice of K is important. A small value of K may lead to overfitting (sensitive to noise), while a large value of K may lead to underfitting (overly smooth predictions). Overall, KNN is simple, intuitive, and effective, especially when the dataset is small and well-structured.\n"
      ],
      "metadata": {
        "id": "5FfEsDJIDzEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "The **Curse of Dimensionality** refers to the problems that arise when working with data in very high-dimensional spaces (that is, when the number of features is large). As the number of dimensions increases, the volume of the feature space grows exponentially, and the data points become increasingly sparse. This sparsity makes it difficult for machine learning algorithms to find meaningful patterns because the notion of “closeness” or similarity between points becomes less reliable.\n",
        "\n",
        "In the context of **K-Nearest Neighbors (KNN)**, the curse of dimensionality significantly affects performance because KNN relies entirely on distance calculations to identify the nearest neighbors. In high-dimensional spaces, the distance between any two points tends to become very similar. The difference between the nearest and farthest neighbor becomes small, making it hard to distinguish which points are truly close. As a result, KNN may select neighbors that are not genuinely similar, leading to poor predictions.\n",
        "\n",
        "Additionally, with more dimensions, more data is required to maintain the same level of accuracy. If the dataset is not large enough, the model may overfit or perform inconsistently. Computational cost also increases because distance calculations must be performed across many features.\n",
        "\n",
        "Overall, the curse of dimensionality reduces the effectiveness of distance-based algorithms like KNN by making distance measures less meaningful, increasing data sparsity, and raising computational complexity. Dimensionality reduction techniques such as feature selection or Principal Component Analysis (PCA) are often used to mitigate this problem.\n"
      ],
      "metadata": {
        "id": "FYBe91HGEkB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much variance (information) as possible. PCA works by transforming the original correlated features into a new set of uncorrelated variables called **principal components**. These components are linear combinations of the original features and are arranged in such a way that the first principal component captures the maximum variance in the data, the second captures the next highest variance, and so on. By selecting only the top few principal components, we can reduce dimensionality while retaining most of the important information in the dataset.\n",
        "\n",
        "PCA is different from **feature selection** in a fundamental way. Feature selection chooses a subset of the original features based on certain criteria (such as correlation, importance scores, or statistical tests). It does not modify the original features; it simply removes the less important ones. In contrast, PCA does not select existing features. Instead, it creates entirely new features (principal components) by combining the original ones. Therefore, PCA is a **feature extraction** method, while feature selection is a **feature reduction** method.\n",
        "\n",
        "In summary, PCA transforms the feature space into a new lower-dimensional space by creating new variables, whereas feature selection keeps some of the original variables and discards others. PCA focuses on capturing maximum variance, while feature selection focuses on identifying and retaining the most relevant original features.\n"
      ],
      "metadata": {
        "id": "JCTCOPOLE6vR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "In **Principal Component Analysis (PCA)**, eigenvalues and eigenvectors are mathematical concepts derived from the covariance matrix of the dataset, and they play a central role in determining the principal components.\n",
        "\n",
        "An **eigenvector** represents a direction in the feature space along which the data varies the most. In PCA, each eigenvector corresponds to a principal component. These vectors define the new axes (directions) onto which the original data is projected. The first eigenvector points in the direction of maximum variance in the data, the second eigenvector points in the direction of the next highest variance (orthogonal to the first), and so on.\n",
        "\n",
        "An **eigenvalue** represents the amount of variance captured along its corresponding eigenvector. In other words, it tells us how important that principal component is. A larger eigenvalue means that the principal component explains a greater portion of the total variance in the dataset.\n",
        "\n",
        "Eigenvalues and eigenvectors are important in PCA because they help in identifying the most informative directions in the data. By ranking eigenvectors based on their eigenvalues (from highest to lowest), we can select the top principal components that capture most of the variance. This allows us to reduce the dimensionality of the dataset while retaining as much useful information as possible. Without eigenvalues and eigenvectors, PCA would not be able to determine which directions preserve the most significant patterns in the data.\n"
      ],
      "metadata": {
        "id": "LPwdcVsTFI8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) complement each other effectively when used together in a single machine learning pipeline, especially for high-dimensional datasets.\n",
        "\n",
        "KNN is a distance-based algorithm that relies heavily on calculating distances between data points. However, in high-dimensional spaces, the **curse of dimensionality** makes distance measures less meaningful, and KNN performance can degrade. This is where PCA becomes useful. PCA reduces the number of features by transforming the original data into a smaller set of principal components that retain most of the important variance. By reducing dimensionality, PCA removes noise, redundant features, and correlations among variables, which helps improve the reliability of distance calculations.\n",
        "\n",
        "When PCA is applied before KNN in a pipeline, it simplifies the feature space and often improves classification or regression accuracy. It also reduces computational cost because KNN has fewer dimensions to process when calculating distances. Additionally, since PCA creates uncorrelated components, it can enhance KNN’s performance by focusing on the most informative directions in the data.\n",
        "\n",
        "PCA improves KNN by reducing dimensionality, minimizing noise, and making distance metrics more meaningful, while KNN benefits from a cleaner and more compact feature space. Together, they create a more efficient and often more accurate machine learning pipeline.\n"
      ],
      "metadata": {
        "id": "2cXB8vYxFcO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "LHuz61ICF28j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# KNN without Feature Scaling\n",
        "# ----------------------------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# ----------------------------\n",
        "# KNN with Feature Scaling\n",
        "# ----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without Scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with Scaling:\", accuracy_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9Wv_Ws3GKFG",
        "outputId": "88d50880-16d6-40cd-fca0-de8aee9a2eb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.7407407407407407\n",
            "Accuracy with Scaling: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "Yg-LSThsGW4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Feature scaling (important before PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eyQDQLTGcX4",
        "outputId": "bb41280d-b8b0-45b9-c558-bfff9d91bba6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "OPuedS3fGlNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Scaling\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN on Original Scaled Data\n",
        "# -----------------------------\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Apply PCA (Top 2 Components)\n",
        "# -----------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN on PCA-Transformed Data\n",
        "# -----------------------------\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# -----------------------------\n",
        "# Print Results\n",
        "# -----------------------------\n",
        "print(\"Accuracy with Original Features:\", accuracy_original)\n",
        "print(\"Accuracy with PCA (Top 2 Components):\", accuracy_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2tmoH3uGozd",
        "outputId": "9ca6d79a-7340-4121-fafe-0baf74605d14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Original Features: 0.9629629629629629\n",
            "Accuracy with PCA (Top 2 Components): 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "c305pYPDG8oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Euclidean Distance\n",
        "# -----------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -----------------------------\n",
        "# KNN with Manhattan Distance\n",
        "# -----------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print Results\n",
        "print(\"Accuracy with Euclidean Distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan Distance:\", accuracy_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rHog-tYHQil",
        "outputId": "17d8d66d-3ff8-49a5-da81-a7013144fc92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9629629629629629\n",
            "Accuracy with Manhattan Distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit. Explain how you would:\n",
        "- Use PCA to reduce dimensionality\n",
        "- Decide how many components to keep\n",
        "- Use KNN for classification post-dimensionality reduction\n",
        "- Evaluate the model\n",
        "- Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "\n",
        "\n",
        "1. Use PCA to Reduce Dimensionality\n",
        "Gene expression datasets often contain thousands of genes (features), many of which are correlated or noisy. PCA helps by:\n",
        "\n",
        "Transforming correlated genes into uncorrelated principal components.\n",
        "\n",
        "Capturing maximum variance in fewer dimensions.\n",
        "\n",
        "Reducing noise and redundancy.\n",
        "\n",
        "Making distance-based models like KNN more reliable.\n",
        "\n",
        "Before applying PCA:\n",
        "\n",
        "Standardize the data (important because PCA is variance-based).\n",
        "\n",
        "Fit PCA on training data only (to avoid data leakage).\n",
        "\n",
        "----\n",
        "2. Decide How Many Components to Keep\n",
        "\n",
        "We can decide the number of components using:\n",
        "\n",
        "Explained Variance Ratio\n",
        "\n",
        "Cumulative variance threshold (e.g., retain 95% variance)\n",
        "\n",
        "Scree plot (elbow method)\n",
        "\n",
        "For biomedical data, retaining 90–95% variance is common because:\n",
        "\n",
        "We preserve biological signal.\n",
        "\n",
        "We remove noise.\n",
        "\n",
        "We drastically reduce dimensionality.\n",
        "\n",
        "---\n",
        "3. Use KNN for Classification After PCA\n",
        "\n",
        "KNN is appropriate because:\n",
        "\n",
        "It is non-parametric (no strong assumptions).\n",
        "\n",
        "Works well when dimensionality is reduced.\n",
        "\n",
        "Makes decisions based on similarity — biologically intuitive for gene expression patterns.\n",
        "\n",
        "After PCA:\n",
        "\n",
        "Train KNN on transformed data.\n",
        "\n",
        "Tune k using cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "4. Evaluate the Model\n",
        "\n",
        "Since biomedical datasets are small:\n",
        "\n",
        "Use Stratified Cross-Validation\n",
        "\n",
        "Measure:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision\n",
        "\n",
        "Recall\n",
        "\n",
        "F1-score\n",
        "\n",
        "Possibly ROC-AUC (for binary cancer classification)\n",
        "\n",
        "Cross-validation reduces optimistic bias and improves reliability."
      ],
      "metadata": {
        "id": "CFPegB7qHi0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Simulate high-dimensional dataset (100 samples, 1000 features)\n",
        "X, y = make_classification(\n",
        "    n_samples=100,\n",
        "    n_features=1000,\n",
        "    n_informative=50,\n",
        "    n_redundant=100,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Build PCA + KNN pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),  # retain 95% variance\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# Train model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Cross-validation accuracy\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Cross-Validation Accuracy:\", np.mean(cv_scores))\n",
        "print(\"Number of PCA Components Retained:\",\n",
        "      pipeline.named_steps['pca'].n_components_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me_4jjUFHzAZ",
        "outputId": "405c8816-5c7f-4827-95f5-f66b4f7d7f69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.43333333333333335\n",
            "Cross-Validation Accuracy: 0.54\n",
            "Number of PCA Components Retained: 63\n"
          ]
        }
      ]
    }
  ]
}